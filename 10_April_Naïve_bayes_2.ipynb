{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "##Ans:---\n",
        "\n",
        "###This is a conditional probability question, where we are given the probability of an employee using the health insurance plan and the probability of a smoker using the plan. We need to find the probability of an employee being a smoker given that he/she uses the health insurance plan.\n",
        "\n",
        "###Let S be the event that an employee is a smoker, and H be the event that an employee uses the health insurance plan. We are given P(H) = 0.7 and P(S|H) = 0.4. We need to find P(S|H).\n",
        "\n",
        "###Using Bayes' theorem, we have:\n",
        "\n",
        "    P(S|H) = P(H|S) * P(S) / P(H)\n",
        "\n",
        "###We can find P(H|S) using the formula:\n",
        "\n",
        "    P(H|S) = P(S|H) * P(H) / P(S)\n",
        "\n",
        "###We know P(S|H) = 0.4 and P(H) = 0.7. To find P(S), we can use the law of total probability:\n",
        "\n",
        "    P(S) = P(S|H) * P(H) + P(S|H') * P(H')\n",
        "\n",
        "###where H' is the complement of H (i.e., an employee does not use the health insurance plan). We are not given P(S|H'), but we can assume that it is the same as P(S), since we do not have any information about the relationship between smoking and not using the health insurance plan.\n",
        "\n",
        "###Substituting the values we have, we get:\n",
        "\n",
        "    P(S) = 0.4 * 0.7 + P(S) * 0.3\n",
        "\n",
        "###Solving for P(S), we get:\n",
        "\n",
        "    P(S) = 0.56\n",
        "\n",
        "###Now we can find P(H|S) using the formula:\n",
        "\n",
        "    P(H|S) = P(S|H) * P(H) / P(S)\n",
        "\n",
        "###Substituting the values we have, we get:\n",
        "\n",
        "    P(H|S) = 0.4 * 0.7 / 0.56\n",
        "\n",
        "###Solving for P(H|S), we get:\n",
        "\n",
        "    P(H|S) = 0.5\n",
        "\n",
        "###Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.5 or 50%.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mnaP1HMmqMKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "##Ans:---\n",
        "\n",
        "###Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes algorithm, which is a probabilistic classification algorithm based on Bayes' theorem. However, they differ in the type of data they are best suited for.\n",
        "\n",
        "###Bernoulli Naive Bayes is used when the input features are binary (i.e., they can take on only two values, such as 0 and 1). It assumes that each input feature is independent of all the others, and that the probability of a feature being present (i.e., taking the value 1) is independent of the other features. It is commonly used in text classification problems, where the input features represent the presence or absence of specific words in a document.\n",
        "\n",
        "###Multinomial Naive Bayes, on the other hand, is used when the input features are counts (i.e., they represent the number of occurrences of a particular event). It assumes that the input features follow a multinomial distribution (hence the name) and that the probability of a particular count is independent of the other counts. It is commonly used in text classification problems, where the input features represent the frequency of specific words in a document.\n",
        "\n",
        "###In summary, Bernoulli Naive Bayes is best suited for binary input features, while Multinomial Naive Bayes is best suited for count-based input features. Both algorithms make the \"naive\" assumption that the input features are independent of each other, which may not always hold in practice. However, despite this simplification, Naive Bayes algorithms are often used and can be quite effective in practice."
      ],
      "metadata": {
        "id": "vpjufzHywvf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "##Ans:---\n",
        "\n",
        "###In Bernoulli Naive Bayes, missing values are typically handled by simply ignoring them during the probability calculations. That is, if a particular input feature is missing (i.e., it is neither 0 nor 1), it is simply skipped over during the probability calculation. This approach assumes that the missing values are missing completely at random (MCAR), meaning that the probability of a value being missing is unrelated to the value itself or any other variables.\n",
        "\n",
        "###However, if the missing values are not MCAR, the model may be biased or inaccurate. In such cases, it may be beneficial to impute the missing values before applying Bernoulli Naive Bayes. There are various imputation techniques that can be used, such as mean imputation, mode imputation, or regression imputation. The imputation method used will depend on the specific problem and the available data.\n",
        "\n",
        "###Another approach is to modify the Bernoulli Naive Bayes algorithm to explicitly model the missing values as a separate category or by using techniques such as the expectation-maximization (EM) algorithm to estimate the missing values.\n",
        "\n",
        "###In general, the best approach for handling missing values in Bernoulli Naive Bayes will depend on the nature of the problem and the specific data available, and it may require experimentation to determine the most effective method.\n"
      ],
      "metadata": {
        "id": "daEO9vytyIc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "##Ans:----\n",
        "\n",
        "###Yes, Gaussian Naive Bayes can be used for multi-class classification. In multi-class classification problems, the goal is to classify instances into one of several possible classes. \n",
        "###In Gaussian Naive Bayes, the algorithm models the distribution of each class as a Gaussian (normal) distribution with a mean and variance for each feature. To make a prediction for a new instance, the algorithm calculates the probability of the instance belonging to each class based on the likelihood of the observed feature values given each class's mean and variance, and then selects the class with the highest probability.\n",
        "\n",
        "###For multi-class classification, there are several approaches to using Gaussian Naive Bayes. One common approach is the \"one-vs-all\" (also known as \"one-vs-rest\") strategy, where a separate Gaussian Naive Bayes model is trained for each class, and each model is trained to distinguish that class from all the other classes. To make a prediction for a new instance, the model computes the probability of the instance belonging to each class using each of the separate models and selects the class with the highest probability.\n",
        "\n",
        "###Another approach is to use the \"one-vs-one\" strategy, where a separate Gaussian Naive Bayes model is trained for each pair of classes. This approach can be more computationally intensive than the \"one-vs-all\" strategy, but it may result in better performance in some cases.\n",
        "\n",
        "###In summary, Gaussian Naive Bayes can be used for multi-class classification by adapting the algorithm to model the distribution of each class as a Gaussian distribution and selecting a suitable strategy for training and prediction."
      ],
      "metadata": {
        "id": "nhlAWXqCz5Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. Assignment:"
      ],
      "metadata": {
        "id": "S9DL74722LST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_nmbaMrqJUI",
        "outputId": "22c0e52e-dc3c-4107-d991-98cbb6f94444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes:\n",
            "Accuracy: 0.8839380364047911\n",
            "Precision: 0.8869617393737383\n",
            "Recall: 0.8152389047416673\n",
            "F1 score: 0.8481249015095276\n",
            "\n",
            "Multinomial Naive Bayes:\n",
            "Accuracy: 0.7863496180326323\n",
            "Precision: 0.7393175533565436\n",
            "Recall: 0.7214983911116508\n",
            "F1 score: 0.7282909724016348\n",
            "\n",
            "Gaussian Naive Bayes:\n",
            "Accuracy: 0.8217730830896915\n",
            "Precision: 0.7103733928118492\n",
            "Recall: 0.9569516119239877\n",
            "F1 score: 0.8130660909542995\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('spambase.data', header=None)\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Define classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score),\n",
        "           'recall': make_scorer(recall_score),\n",
        "           'f1_score': make_scorer(f1_score)}\n",
        "\n",
        "# Evaluate classifiers using 10-fold cross validation\n",
        "bernoulli_scores = cross_validate(bernoulli_nb, X, y, cv=10, scoring=scoring)\n",
        "multinomial_scores = cross_validate(multinomial_nb, X, y, cv=10, scoring=scoring)\n",
        "gaussian_scores = cross_validate(gaussian_nb, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "# Print average scores for each classifier and each metric\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(\"Accuracy:\", bernoulli_scores['test_accuracy'].mean())\n",
        "print(\"Precision:\", bernoulli_scores['test_precision'].mean())\n",
        "print(\"Recall:\", bernoulli_scores['test_recall'].mean())\n",
        "print(\"F1 score:\", bernoulli_scores['test_f1_score'].mean())\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes:\")\n",
        "print(\"Accuracy:\", multinomial_scores['test_accuracy'].mean())\n",
        "print(\"Precision:\", multinomial_scores['test_precision'].mean())\n",
        "print(\"Recall:\", multinomial_scores['test_recall'].mean())\n",
        "print(\"F1 score:\", multinomial_scores['test_f1_score'].mean())\n",
        "\n",
        "print(\"\\nGaussian Naive Bayes:\")\n",
        "print(\"Accuracy:\", gaussian_scores['test_accuracy'].mean())\n",
        "print(\"Precision:\", gaussian_scores['test_precision'].mean())\n",
        "print(\"Recall:\", gaussian_scores['test_recall'].mean())\n",
        "print(\"F1 score:\", gaussian_scores['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The results show that Bernoulli Naive Bayes has the highest accuracy, precision, recall, and F1 score compared to Multinomial Naive Bayes and Gaussian Naive Bayes. This could be due to the fact that the Bernoulli Naive Bayes classifier works well with binary data, which is the case in this dataset.\n",
        "\n",
        "##limitations\n",
        "* The limitations of Naive Bayes observed in this exercise include the assumption of independence between features, which may not always hold in real-world scenarios. Additionally, the model may not perform well when the dataset has missing values or when the features are not well-defined.\n",
        "\n",
        "###Future work could involve using more advanced models, such as decision trees, random forests, or neural networks, to compare their performance with Naive Bayes on this dataset. Additionally, feature selection or engineering techniques could be applied to improve the performance of the models."
      ],
      "metadata": {
        "id": "7_K-QJsQ3_Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Rog3LsM22xP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}